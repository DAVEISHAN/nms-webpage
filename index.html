<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Project Webpage for No More Shortcuts: Realizing the Potential of Temporal Self-Supervision.">
  <meta name="keywords" content="Video Self-supervised Learning, SSL, Representation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision </title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/NMS_logo_cropped_small.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
        /* Carousel Specific Styles */
        #image-carousel-container .carousel {
            display: flex;
            width: 400%; /* Adjust based on the number of items */
        }

        #image-carousel-container .carousel-item {
            flex: 0 0 100%; /* Each item is 100% of the carousel's viewport */
        }
    </style>

  <style>
        .table-container {
            border-collapse: collapse;
            width: 100%;
        }
        .table-container th, .table-container td {
            border: 1px solid black; /* Black borders */
            text-align: center;
            padding: 5px;
        }
        .best-result { color: red; font-weight: bold; }
        .second-best { color: blue; text-decoration: underline; }
    </style>

  
</head>
  
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
        <h1 class="title is-1 publication-title">
        No More Shortcuts: Realizing the Potential of Temporal Self-Supervision
        <br>
        <span class="conference-info">AAAI 2024 (Main Technical Track)</span>
      </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daveishan.github.io">Ishan Rajendrakumar Dave</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://sjenni.github.io/">Simon Jenni</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a><sup>1</sup>,
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Center for Research in Computer Vision, University of Central Florida, USA,</span><br>
            <span class="author-block"><sup>2</sup>Adobe Research, USA</span><br>
            <span class="author-block" style="font-size: 0.5em;"><sup>*</sup> Work done as an intern at Adobe Research, USA.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.13008"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
      
             <span class="link-block">
                <a href="https://www.youtube.com/watch?v=5MBnxmMBQh0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="./static/images/final_square_nms_aaai24_poster.jpg"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-image"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
            
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
<!--       <div class="column is-four-fifths"> -->
      <div class="column is-full"> <!-- Changed to is-full for full width -->

       <h3 class="title is-3">TL;DR:</h3>
            <div class="content has-text-justified">
                <p>
                    Temporal pretext tasks are a great way of learning video self-supervised representation; however, they are too easy to solve for Video Transformers. Our contribution is as follows:
                    <ul>
                        <li>Breaking local-patch based shortcuts in the temporal pretext tasks to promote <strong>high-level</strong> motion features (i.e., how an entire actor/object moves in the video), which help in learning both semantic-level action-related features and <strong>low-level</strong> object/pose-related features.</li>
                        <li>Proposing more challenging <strong>frame-level</strong> temporal pretext tasks over traditional clip-level tasks.</li>

                    </ul>
                </p>
            </div>
            
            <h3 class="title is-3">What's new over other Video self-supervised methods?</h3>
            <div class="content has-text-justified">
                <ul>
                    <li>Suitable for both <strong>high-level</strong> semantic tasks like video retrieval, action classification, and video attribute recognition (such as object and scene identification) and <strong>low-level</strong> temporal correspondence tasks like video object segmentation and pose tracking.</li>
                    <li><strong>Does not require full-finetuning</strong> - Absolute SoTA on Video Retrieval and Linear Video Classification. Most suitable for video search applications!</li>
                    <li>Provides amazing frame-level video representations (not just video-level)!</li>
                </ul>
            </div>        

          
      </div>
    </div>

  </div>
</section>
    <!--/ Abstract. -->
    
    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
        
<!--         <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
            Self-supervised approaches for video have shown impressive results in video understanding tasks. However, unlike early works that leverage temporal self-supervision, current state-of-the-art methods primarily rely on tasks from the image domain (e.g., contrastive learning) that do not explicitly promote the learning of temporal features. We identify two factors that limit existing temporal self-supervision: 1) tasks are too simple, resulting in saturated training performance, and 2) we uncover shortcuts based on local appearance statistics that hinder the learning of high-level features. To address these issues, we propose 1) a more challenging reformulation of temporal self-supervision as frame-level (rather than clip-level) recognition tasks and 2) an effective augmentation strategy to mitigate shortcuts. Our model extends a representation of single video frames, pre-trained through contrastive learning, with a transformer that we train through temporal self-supervision. 
          </p>
          <p>
            We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. The generalization capability of our self-supervised video method is evidenced by its state-of-the-art performance in a wide range of high-level semantic tasks, including video retrieval, action classification, and video attribute recognition (such as object and scene identification), as well as low-level temporal correspondence tasks like video object segmentation and pose tracking. Additionally, we show that the video representations learned through our method exhibit increased robustness to the input perturbations.
          </p>
        </div> -->
  

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Low-Level Temporal Correspondence based Downstream Tasks</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Video Object Segmentation (VOS)</h3>
        <div class="content has-text-justified">
          <p>
            We follow the semi-supervised protocol of DAVIS-2017 [43], where the object masks of the first frame of a video are given, and the task is to predict the masks in the rest of the frames. No layers are tuned on this task. 
            frame.
          </p>
        </div>

    <div class="hero-body">
          <div class="container" style="display: flex; width: 100%;">
      
              <!-- Left side - Video Carousel -->
              <div class="carousel-container" style="width: 100%; overflow: hidden;">
                    <div id="carousel" class="carousel" style="display: flex; width: 400%; transition: transform 0.5s ease;">
                        <!-- Video 1 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_dog.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 2 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_car-shadow.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 3 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_parkour.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 4 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/shooting.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
      
              <div style="width: 55%; margin-left: 20px; display: flex; justify-content: center;">
                  <table style="width: 95%; border-collapse: collapse; text-align: center;">
                      <caption>Video Object Segmentation on <strong>DAVIS-2017</strong>.</caption>
                      <thead>
                          <tr>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>Pretraining</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>J&amp;F-Mean</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>J-Mean</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>F-Mean</strong></th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>ST-MAE</td>
                              <td>53.5</td>
                              <td>52.6</td>
                              <td>54.4</td>
                          </tr>
                          <tr>
                              <td>VideoMAE</td>
                              <td>53.8</td>
                              <td>53.2</td>
                              <td>54.4</td>
                          </tr>
                          <tr>
                              <td>MotionMAE</td>
                              <td style="color: blue; text-decoration: underline;">56.8</td>
                              <td style="color: blue; text-decoration: underline;">55.8</td>
                              <td style="color: blue; text-decoration: underline;">57.8</td>
                          </tr>
                          <tr>
                              <td>SVT</td>
                              <td>48.5</td>
                              <td>46.8</td>
                              <td>50.1</td>
                          </tr>
                          <tr style="border-bottom: 2px solid black;">
                              <td>Ours (ViT-B)</td>
                              <td style="color: red; font-weight: bold;">62.1</td>
                              <td style="color: red; font-weight: bold;">60.5</td>
                              <td style="color: red; font-weight: bold;">63.6</td>
                          </tr>
                      </tbody>
                  </table>
              </div>
            
                          
          </div>
      </div>
    

        <br/>
        
      <h3 class="title is-4">Human Pose Propagation</h3>
      <div class="content has-text-justified">
        <p>
          In this protocol, the key points of the human pose are given for the first frame, and the task is to predict the location of those key points in the subsequent frames. We employ pre-trained video SSL model without any further tuning.
        </p>
      </div>   
      
      <div class="hero-body">
            <div class="container" id="image-carousel-container" style="overflow: hidden;">
                <div class="carousel results-carousel" style="display: flex; width: 400%; transition: transform 0.5s ease;">
                    <!-- Image 1 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop0.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 2 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop1.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 3 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop2.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 4 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop3.drawio.png" style="width: 75%; height: auto;">
                    </div>
                </div>
            </div>
        </div>
      

      
       

    
      <div class="content has-text-justified">
        <h2 class="title is-3">High-level Semantics based Downstream Tasks</h2>
        <h3 class="title is-4">Video-to-Video Retrieval</h3>
        <div class="content has-text-justified">
          <p>
            We perform video retrieval experiments to demonstrate the suitability of our features for semantic video similarity search, no finetuning is allowed. Following prior works [9, 13, 21], the test set of each dataset is used as a query-set, and the training set is considered as a search-set. We report Top-1 and Top-5 retrieval accuracy.
            frame.
          </p>
        </div>
        
        <!-- NNR video. -->
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
<!--                 <h2 class="title is-3">Video</h2>
                <div class="publication-video"> -->
                    <video controls autoplay muted loop playsinline style="width: 90%; height: auto;">
                        <source src="./static/videos/animations_nmsv4.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>
        </div>
        <!--/ NNR video. -->
        
        <div class="content has-text-justified">
        <div id="table-container"></div>
        <script>
            fetch('main_table.html')
                .then(response => response.text())
                .then(html => {
                    document.getElementById('table-container').innerHTML = html;
                })
                .catch(error => console.error('Error loading the table:', error));
            </script>

        </div>

        <h3 class="title is-4">Multi-label Video Attributes Recognition</h3>
        <div class="content has-text-justified">
          <p>
            Holistic Video Understanding (HVU) [12] is a large-scale benchmark addressing multi-label and multi-task video understanding of multiple semantic aspects, including scenes, objects, actions, attributes, and concepts. We train different linear classifiers over frozen features and report results in mean average precision (mAP).
          </p>
        </div>
  
        <div class="hero-body">
    <div class="columns">
        <!-- Image Column -->
        <div class="column" style="width: 40%;">
            <img src="./static/images/hvu.drawio.png" style="width: 100%; height: 450px;"> <!-- Adjusted height here -->
            <p style="text-align: center;">Example from HVU dataset.</p> <!-- Center-aligned caption for the image -->
        </div>

        <!-- Table Column -->
        <div class="column" style="width: 60%;">
    <table style="width: 95%; border-collapse: collapse; text-align: center;">
        <caption>
            <div style="text-align: center;"> <!-- Center-aligned caption for the table -->
                <strong>Downstream on HVU dataset.</strong>
            </div>  
        </caption>
        <thead>
            <tr>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Method</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Action</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Obj.</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Scene</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Event</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Attr.</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Concept</th>
                <th style="border-top: 2px solid black; border-bottom: 2px solid black;">Overall</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>SVT</td>
                <td>38.48</td>
                <td>30.35</td>
                <td>30.97</td>
                <td>37.87</td>
                <td>28.20</td>
                <td>35.64</td>
                <td class="second-best">33.58</td>
            </tr>
            <tr>
                <td>&rho;BYOL</td>
                <td>33.20</td>
                <td>25.82</td>
                <td>28.40</td>
                <td>35.50</td>
                <td>24.16</td>
                <td>33.21</td>
                <td>30.05</td>
            </tr>
            <tr>
                <td>VideoMAE</td>
                <td>27.49</td>
                <td>23.36</td>
                <td>24.56</td>
                <td>29.78</td>
                <td>21.04</td>
                <td>28.75</td>
                <td>25.83</td>
            </tr>
            <tr style="border-bottom: 2px solid black;">
                <td>Ours (ViT-B)</td>
                <td>38.65</td>
                <td>33.46</td>
                <td>34.24</td>
                <td>40.23</td>
                <td>30.99</td>
                <td>38.38</td>
                <td class="best-result">35.99</td>
            </tr>
                </tbody>
            </table>
        </div>

      
            </div>
            </div>
    
      </div>
      </div>
      </div>




  </div>
<!-- </section>
  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-level Semantics based Downstream Tasks</h2>
        
      </div>
    </div>
    </div>


  </div>
</section> -->
        

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
     @article{dave2024codamal,
        title={CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost Microscopes},
        author={Dave, Ishan Rajendrakumar and de Blegiers, Tristan and Chen, Chen and Shah, Mubarak},
        journal={arXiv preprint arXiv:2402.10478},
        year={2024}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
<!--     <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
  
<script>
    let currentSlide = 0;

    function moveSlide(direction) {
        const slides = document.querySelectorAll('.carousel-item');
        const totalSlides = slides.length;
        
        // Update current slide based on direction
        if (direction === 'next') {
            currentSlide = (currentSlide + 1) % totalSlides;
        } else if (direction === 'prev') {
            currentSlide = (currentSlide - 1 + totalSlides) % totalSlides;
        }

        // Update carousel position
        document.getElementById('carousel').style.transform = `translateX(-${currentSlide * 100 / totalSlides}%)`;
    }
</script>
  
<script>
    let currentSlide = 0;

    function moveSlide(step) {
        const slides = document.querySelectorAll('.carousel-item');
        const totalSlides = slides.length;
        currentSlide = (currentSlide + step + totalSlides) % totalSlides;
        document.getElementById('carousel').style.transform = `translateX(-${currentSlide * 100}%)`;
    }
</script>
    
<script>
    let currentImageSlide = 0;
    
    function moveImageSlide(step) {
        const slides = document.querySelectorAll('#imageCarousel .carousel-item');
        const totalSlides = slides.length;
        currentImageSlide = (currentImageSlide + step + totalSlides) % totalSlides;
        document.getElementById('imageCarousel').style.transform = `translateX(-${currentImageSlide * 100}%)`;
    }
</script>    
    
<script>
        let currentImage = 0;
        const totalImages = 4; // Total number of images in the carousel

        function cycleImages() {
            currentImage = (currentImage + 1) % totalImages;
            document.getElementById('image-carousel').style.transform = `translateX(-${currentImage * 100 / totalImages}%)`;
        }

        // Set the interval for automatic change
        setInterval(cycleImages, 3000); // Change image every 3 seconds
    </script>
  
</body>
</html>
