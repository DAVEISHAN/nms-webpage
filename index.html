<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Project Webpage for No More Shortcuts: Realizing the Potential of Temporal Self-Supervision.">
  <meta name="keywords" content="Video Self-supervised Learning, SSL, Representation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision </title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/NMS_logo_cropped_small.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
        /* Carousel Specific Styles */
        #image-carousel-container .carousel {
            display: flex;
            width: 400%; /* Adjust based on the number of items */
        }

        #image-carousel-container .carousel-item {
            flex: 0 0 100%; /* Each item is 100% of the carousel's viewport */
        }
    </style>

  
</head>
  
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
        <h1 class="title is-1 publication-title">
        No More Shortcuts: Realizing the Potential of Temporal Self-Supervision
        <br>
        <span class="conference-info">AAAI 2024 (Main Technical Track)</span>
      </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daveishan.github.io">Ishan Rajendrakumar Dave</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://sjenni.github.io/">Simon Jenni</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a><sup>1</sup>,
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Center for Research in Computer Vision, University of Central Florida, USA,</span><br>
            <span class="author-block"><sup>2</sup>Adobe Research, USA</span><br>
            <span class="author-block" style="font-size: 0.5em;"><sup>*</sup> Work done as an intern at Adobe Research, USA.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
<!--               <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2312.13008"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--               <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">This webpage is still under construction!</h2>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
            Self-supervised approaches for video have shown impressive results in video understanding tasks. However, unlike early works that leverage temporal self-supervision, current state-of-the-art methods primarily rely on tasks from the image domain (e.g., contrastive learning) that do not explicitly promote the learning of temporal features. We identify two factors that limit existing temporal self-supervision: 1) tasks are too simple, resulting in saturated training performance, and 2) we uncover shortcuts based on local appearance statistics that hinder the learning of high-level features. To address these issues, we propose 1) a more challenging reformulation of temporal self-supervision as frame-level (rather than clip-level) recognition tasks and 2) an effective augmentation strategy to mitigate shortcuts. Our model extends a representation of single video frames, pre-trained through contrastive learning, with a transformer that we train through temporal self-supervision. 
          </p>
          <p>
            We demonstrate experimentally that our more challenging frame-level task formulations and the removal of shortcuts drastically improve the quality of features learned through temporal self-supervision. The generalization capability of our self-supervised video method is evidenced by its state-of-the-art performance in a wide range of high-level semantic tasks, including video retrieval, action classification, and video attribute recognition (such as object and scene identification), as well as low-level temporal correspondence tasks like video object segmentation and pose tracking. Additionally, we show that the video representations learned through our method exhibit increased robustness to the input perturbations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Low-Level Temporal Correspondence based Downstream Tasks</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Video Object Segmentation (VOS)</h3>
        <div class="content has-text-justified">
          <p>
            We follow the semi-supervised protocol of DAVIS-2017 [43], where the object masks of the first frame of a video are given, and the task is to predict the masks in the rest of the frames. No layers are tuned on this task. 
            frame.
          </p>
        </div>

    <div class="hero-body">
          <div class="container" style="display: flex; width: 100%;">
      
              <!-- Left side - Video Carousel -->
              <div class="carousel-container" style="width: 100%; overflow: hidden;">
                    <div id="carousel" class="carousel" style="display: flex; width: 400%; transition: transform 0.5s ease;">
                        <!-- Video 1 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_dog.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 2 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_car-shadow.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 3 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/vos_parkour.mp4" type="video/mp4">
                            </video>
                        </div>
                        <!-- Video 4 -->
                        <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box; display: flex; justify-content: center;">
                            <video controls muted loop playsinline style="width: 120%; height: auto;">
                                <source src="./static/videos/shooting.mp4" type="video/mp4">
                            </video>
                        </div>
                    </div>
                </div>
      
              <div style="width: 55%; margin-left: 20px; display: flex; justify-content: center;">
                  <table style="width: 95%; border-collapse: collapse; text-align: center;">
                      <caption>Video Object Segmentation on <strong>DAVIS-2017</strong>.</caption>
                      <thead>
                          <tr>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>Pretraining</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>J&amp;F-Mean</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>J-Mean</strong></th>
                              <th style="border-top: 2px solid black; border-bottom: 2px solid black;"><strong>F-Mean</strong></th>
                          </tr>
                      </thead>
                      <tbody>
                          <tr>
                              <td>ST-MAE</td>
                              <td>53.5</td>
                              <td>52.6</td>
                              <td>54.4</td>
                          </tr>
                          <tr>
                              <td>VideoMAE</td>
                              <td>53.8</td>
                              <td>53.2</td>
                              <td>54.4</td>
                          </tr>
                          <tr>
                              <td>MotionMAE</td>
                              <td style="color: blue; text-decoration: underline;">56.8</td>
                              <td style="color: blue; text-decoration: underline;">55.8</td>
                              <td style="color: blue; text-decoration: underline;">57.8</td>
                          </tr>
                          <tr>
                              <td>SVT</td>
                              <td>48.5</td>
                              <td>46.8</td>
                              <td>50.1</td>
                          </tr>
                          <tr style="border-bottom: 2px solid black;">
                              <td>Ours (ViT-B)</td>
                              <td style="color: red; font-weight: bold;">62.1</td>
                              <td style="color: red; font-weight: bold;">60.5</td>
                              <td style="color: red; font-weight: bold;">63.6</td>
                          </tr>
                      </tbody>
                  </table>
              </div>
            
                          
          </div>
      </div>
    

        <br/>
        
      <h3 class="title is-4">Human Pose Propagation</h3>
      <div class="content has-text-justified">
        <p>
          In this protocol, the key points of the human pose are given for the first frame, and the task is to predict the location of those key points in the subsequent frames. We employ pre-trained video SSL model without any further tuning.
        </p>
      </div>   
      
      <div class="hero-body">
            <div class="container" id="image-carousel-container" style="overflow: hidden;">
                <div class="carousel results-carousel" style="display: flex; width: 400%; transition: transform 0.5s ease;">
                    <!-- Image 1 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop0.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 2 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop1.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 3 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop2.drawio.png" style="width: 75%; height: auto;">
                    </div>
                    <!-- Image 4 -->
                    <div class="carousel-item" style="flex: 0 0 25%; box-sizing: border-box;">
                        <img src="./static/images/pose_prop/pose_prop3.drawio.png" style="width: 75%; height: auto;">
                    </div>
                </div>
            </div>
        </div>


      
       

    
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-level Semantics based Downstream Tasks</h2>
        
      </div>
    

      </div>
      </div>




  </div>
<!-- </section>
  
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">High-level Semantics based Downstream Tasks</h2>
        
      </div>
    </div>
    </div>


  </div>
</section> -->
        

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{dave2024nomore,
  title={No More Shortcuts: Realizing the Potential of Temporal Self-Supervision},
  author={Dave, Ishan and Jenni, Simon and Shah, Mubarak},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
<!--     <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
  
<script>
    let currentSlide = 0;

    function moveSlide(direction) {
        const slides = document.querySelectorAll('.carousel-item');
        const totalSlides = slides.length;
        
        // Update current slide based on direction
        if (direction === 'next') {
            currentSlide = (currentSlide + 1) % totalSlides;
        } else if (direction === 'prev') {
            currentSlide = (currentSlide - 1 + totalSlides) % totalSlides;
        }

        // Update carousel position
        document.getElementById('carousel').style.transform = `translateX(-${currentSlide * 100 / totalSlides}%)`;
    }
</script>
  
<script>
    let currentSlide = 0;

    function moveSlide(step) {
        const slides = document.querySelectorAll('.carousel-item');
        const totalSlides = slides.length;
        currentSlide = (currentSlide + step + totalSlides) % totalSlides;
        document.getElementById('carousel').style.transform = `translateX(-${currentSlide * 100}%)`;
    }
</script>
    
<script>
    let currentImageSlide = 0;
    
    function moveImageSlide(step) {
        const slides = document.querySelectorAll('#imageCarousel .carousel-item');
        const totalSlides = slides.length;
        currentImageSlide = (currentImageSlide + step + totalSlides) % totalSlides;
        document.getElementById('imageCarousel').style.transform = `translateX(-${currentImageSlide * 100}%)`;
    }
</script>    
    
<script>
        let currentImage = 0;
        const totalImages = 4; // Total number of images in the carousel

        function cycleImages() {
            currentImage = (currentImage + 1) % totalImages;
            document.getElementById('image-carousel').style.transform = `translateX(-${currentImage * 100 / totalImages}%)`;
        }

        // Set the interval for automatic change
        setInterval(cycleImages, 3000); // Change image every 3 seconds
    </script>
  
</body>
</html>
